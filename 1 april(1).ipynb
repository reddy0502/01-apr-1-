{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b03e5c9e-9b9c-4ee2-87bb-891ccae3b805",
   "metadata": {},
   "source": [
    "1ans:\n",
    "\n",
    "Linear regression is used to predict a continuous outcome variable based on one or more predictor variables. For example, a linear regression model could be used to predict a person's weight based on their height, age, and gender. Linear regression assumes that the relationship between the predictor variables and the outcome variable is linear and that the errors are normally distributed.\n",
    "\n",
    "Logistic regression, on the other hand, is used to predict a categorical outcome variable based on one or more predictor variables. For example, a logistic regression model could be used to predict whether a person will purchase a product based on their age, income, and gender. Logistic regression assumes that the relationship between the predictor variables and the outcome variable is non-linear and that the errors are not normally distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8031b43-e28a-44a6-82a4-fc175a5dd70b",
   "metadata": {},
   "source": [
    "2ans:\n",
    "\n",
    "The cost function used in logistic regression is the logarithmic loss or cross-entropy loss function. The goal of logistic regression is to minimize the difference between the predicted probabilities and the actual class labels. The logarithmic loss function measures the difference between the predicted probability and the actual class label for each training example, and then averages those differences over the entire training set.\n",
    "\n",
    "The formula for the logistic regression cost function is:\n",
    "\n",
    "\n",
    "J(θ) = (-1/m) * Σ [y*log(h(x)) + (1-y)*log(1-h(x))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcaf008-5531-417a-ad6b-88dfe5cca3fd",
   "metadata": {},
   "source": [
    "3ans:\n",
    "\n",
    "Regularization is a technique used in logistic regression to prevent overfitting of the model by adding a penalty term to the cost function. Overfitting occurs when the model fits the training data too closely and captures the noise in the data, leading to poor performance on new, unseen data.\n",
    "\n",
    "Regularization helps prevent overfitting by discouraging the model from assigning too much importance to any one feature. By adding a penalty term to the cost function, the model is incentivized to choose parameter values that are small and spread across multiple features, rather than large values that may only fit the noise in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acd9a46-19db-4798-9d26-a398c9bc49c2",
   "metadata": {},
   "source": [
    "4ans:\n",
    "\n",
    "The Receiver Operating Characteristic (ROC) curve is a graphical representation of the performance of a binary classifier, such as a logistic regression model. The ROC curve plots the true positive rate (TPR) against the false positive rate (FPR) for different classification thresholds.\n",
    "\n",
    "The area under the ROC curve (AUC) is a commonly used metric to evaluate the performance of a binary classifier. A higher AUC indicates a better classifier performance. An AUC of 0.5 indicates a random classifier, while an AUC of 1 indicates a perfect classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c25e24-29a6-456b-97a5-862f847b3832",
   "metadata": {},
   "source": [
    "5ans:\n",
    "\n",
    "There are several techniques for feature selection in logistic regression, including:\n",
    "\n",
    "Univariate feature selection: This technique selects features based on their individual relationship with the target variable, using statistical tests such as chi-squared, ANOVA, or mutual information. Features with the highest test scores are selected.\n",
    "\n",
    "Recursive feature elimination: This technique starts with all features and recursively removes the least important feature until a desired number of features is reached. The importance of a feature is typically measured by the coefficient magnitude or feature weight in the logistic regression model.\n",
    "\n",
    "L1 regularization: As mentioned earlier, L1 regularization can shrink some of the feature weights to zero, effectively removing those features from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8378c1-7951-4b45-8de9-4e6cabfead4b",
   "metadata": {},
   "source": [
    "6ans:\n",
    "\n",
    " Here are some strategies for handling imbalanced datasets in logistic regression:\n",
    "\n",
    "Resampling techniques: Resampling involves either oversampling the minority class or undersampling the majority class to balance the dataset. Oversampling techniques include random oversampling, synthetic minority oversampling technique (SMOTE), and adaptive synthetic (ADASYN) sampling. Undersampling techniques include random undersampling and Tomek links.\n",
    "\n",
    "Class weighting: In logistic regression, assigning higher weight to the minority class can improve the model's ability to predict the minority class. This can be achieved by using class weighting techniques such as inverse frequency weighting or balanced class weighting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2722718a-2641-413f-b047-3c0843dd965e",
   "metadata": {},
   "source": [
    "7ans:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
